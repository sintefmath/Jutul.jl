<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Sensitivities, adjoints and optimization · Jutul.jl</title><meta name="title" content="Sensitivities, adjoints and optimization · Jutul.jl"/><meta property="og:title" content="Sensitivities, adjoints and optimization · Jutul.jl"/><meta property="twitter:title" content="Sensitivities, adjoints and optimization · Jutul.jl"/><meta name="description" content="Documentation for Jutul.jl."/><meta property="og:description" content="Documentation for Jutul.jl."/><meta property="twitter:description" content="Documentation for Jutul.jl."/><meta property="og:url" content="https://sintefmath.github.io/Jutul.jl/optimization/"/><meta property="twitter:url" content="https://sintefmath.github.io/Jutul.jl/optimization/"/><link rel="canonical" href="https://sintefmath.github.io/Jutul.jl/optimization/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">Jutul.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../usage/">Usage</a></li><li class="is-active"><a class="tocitem" href>Sensitivities, adjoints and optimization</a><ul class="internal"><li><a class="tocitem" href="#Objective-functions"><span>Objective functions</span></a></li><li><a class="tocitem" href="#Generic-optimization-interface"><span>Generic optimization interface</span></a></li><li><a class="tocitem" href="#Numerical-parameter-optimization-interface"><span>Numerical parameter optimization interface</span></a></li></ul></li><li><a class="tocitem" href="../mesh/">Meshes and mesh utilities</a></li><li><a class="tocitem" href="../units/">Units</a></li><li><a class="tocitem" href="../internals/">Internals</a></li><li><a class="tocitem" href="../docstrings/">Docstrings</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Sensitivities, adjoints and optimization</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Sensitivities, adjoints and optimization</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/sintefmath/Jutul.jl/blob/main/docs/src/optimization.md#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Sensitivities,-adjoints-and-optimization"><a class="docs-heading-anchor" href="#Sensitivities,-adjoints-and-optimization">Sensitivities, adjoints and optimization</a><a id="Sensitivities,-adjoints-and-optimization-1"></a><a class="docs-heading-anchor-permalink" href="#Sensitivities,-adjoints-and-optimization" title="Permalink"></a></h1><p>Jutul.jl is build from the ground up to support gradient-based optimization problems. This includes both data assimilation/parameter calibration and control problems.</p><p>An example application from <code>JutulDarcy.jl</code> demonstrates many of these functions: <a href="https://sintefmath.github.io/JutulDarcy.jl/dev/examples/workflow/fully_differentiable_geothermal">A fully differentiable geothermal doublet: History matching and control optimization</a></p><h2 id="Objective-functions"><a class="docs-heading-anchor" href="#Objective-functions">Objective functions</a><a id="Objective-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Objective-functions" title="Permalink"></a></h2><p>There are two main types of objective functions supported in Jutul: Those that evaluated globally over all time-steps, and those who are evaluated locally (typically as a sum over all time-steps).</p><article><details class="docstring" open="true"><summary id="Jutul.AbstractJutulObjective"><a class="docstring-binding" href="#Jutul.AbstractJutulObjective"><code>Jutul.AbstractJutulObjective</code></a> — <span class="docstring-category">Type</span></summary><section><div><p>Abstract type for Jutul objectives.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sintefmath/Jutul.jl/blob/61db0547d056b6ca2f8db5edc1f198cb8015c64a/src/core_types/core_types.jl#L1528-L1530">source</a></section></details></article><p>For either type of objective, you must implement the correct interface. This can either be done by passing a function directly, with Jutul guessing from the number of arguments what kind of objective it is, or by explicitly making a subtype that is <a href="https://docs.julialang.org/en/v1/manual/methods/#Function-like-objects">a Julia callable struct</a>.</p><p>These functions take in the <code>step_info</code> <code>Dict</code>, which is worth having a look at if you plan on writing objective functions:</p><article><details class="docstring" open="true"><summary id="Jutul.optimization_step_info"><a class="docstring-binding" href="#Jutul.optimization_step_info"><code>Jutul.optimization_step_info</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">optimization_step_info(step::Int, time::Real, dt; kwarg...)</code></pre><p>Optimization step information is normally not set up manually, but the output from this function will be passed to objective functions as <code>step_info</code>. This function is a <code>Dict</code> with the following fields:</p><p><strong>Fields</strong></p><ul><li><code>:time</code> - The time at the end of the step. To get time at the start of the step, use <code>step_info[:time] - step_info[:dt]</code></li><li><code>:dt</code> - The time step size for this step.</li><li><code>:step</code> - The step number, starting at 1. Not that this is the report step, and multiple <code>step_info</code> entries could have the same step if substeps are used.</li><li><code>:Nstep</code> - The total number of steps in the simulation. </li><li><code>:substep</code> - The substep number, starting at 1. This is used to indicate that multiple steps are taken within a single step.</li><li><code>:substep_global</code> - The global substep number, starting at 1 and will not reset between global steps.</li><li><code>:Nsubstep_global</code> - The total number of substeps in the simulation.</li><li><code>:total_time</code> - The total time of the simulation (i.e. time at the final step and substep)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sintefmath/Jutul.jl/blob/61db0547d056b6ca2f8db5edc1f198cb8015c64a/src/ad/gradients.jl#L1031-L1053">source</a></section></details></article><h3 id="Global-objectives"><a class="docs-heading-anchor" href="#Global-objectives">Global objectives</a><a id="Global-objectives-1"></a><a class="docs-heading-anchor-permalink" href="#Global-objectives" title="Permalink"></a></h3><p>These are objectives that are defined over the entire simulation time. This means that you define the objective function as a single function that takes in the solution for all time-steps together with forces, time-step information, initial state and input data used to set up the model (if any).</p><article><details class="docstring" open="true"><summary id="Jutul.AbstractGlobalObjective"><a class="docstring-binding" href="#Jutul.AbstractGlobalObjective"><code>Jutul.AbstractGlobalObjective</code></a> — <span class="docstring-category">Type</span></summary><section><div><p>Abstract type for objective as a global objective function on the form:</p><pre><code class="language-julia hljs">F(model, state0, states, step_infos, forces, input_data)</code></pre><p>Note that if substeps are enabled by setting <code>output_substates=true</code> in the simulator setup, the length of forces and states will be dynamic and of equal length to <code>step_infos</code>. If you want to recover the state at a specific step, you can reason about <code>:step</code> field of the corresponding entry in the <code>step_infos</code> array.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sintefmath/Jutul.jl/blob/61db0547d056b6ca2f8db5edc1f198cb8015c64a/src/core_types/core_types.jl#L1546-L1556">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jutul.WrappedGlobalObjective"><a class="docstring-binding" href="#Jutul.WrappedGlobalObjective"><code>Jutul.WrappedGlobalObjective</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">WrappedGlobalObjective(objective)</code></pre><p>A global (in time) objective that wraps a function/callable. This type automatically wraps functions/callable structs that are passed to the optimization interface if they have the sum signature (five arguments).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sintefmath/Jutul.jl/blob/61db0547d056b6ca2f8db5edc1f198cb8015c64a/src/core_types/core_types.jl#L1576-L1582">source</a></section></details></article><h3 id="Local/sum-objectives"><a class="docs-heading-anchor" href="#Local/sum-objectives">Local/sum objectives</a><a id="Local/sum-objectives-1"></a><a class="docs-heading-anchor-permalink" href="#Local/sum-objectives" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="Jutul.AbstractSumObjective"><a class="docstring-binding" href="#Jutul.AbstractSumObjective"><code>Jutul.AbstractSumObjective</code></a> — <span class="docstring-category">Type</span></summary><section><div><p>Abstract type for objective as a sum of function values on the form:</p><pre><code class="language-julia hljs">F(model, state, dt, step_info, forces)</code></pre><p>evaluated for each step. This means that the objective is a sum of all of these values. If you want to only depend on a single step, you can look up</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sintefmath/Jutul.jl/blob/61db0547d056b6ca2f8db5edc1f198cb8015c64a/src/core_types/core_types.jl#L1536-L1543">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jutul.WrappedSumObjective"><a class="docstring-binding" href="#Jutul.WrappedSumObjective"><code>Jutul.WrappedSumObjective</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">WrappedSumObjective(objective)</code></pre><p>An objective that is a sum of function values, evaluated for each step, defined by a function. This type automatically wraps functions/callable structs that are passed to the optimization interface if they have the sum signature (five arguments).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sintefmath/Jutul.jl/blob/61db0547d056b6ca2f8db5edc1f198cb8015c64a/src/core_types/core_types.jl#L1559-L1566">source</a></section></details></article><h2 id="Generic-optimization-interface"><a class="docs-heading-anchor" href="#Generic-optimization-interface">Generic optimization interface</a><a id="Generic-optimization-interface-1"></a><a class="docs-heading-anchor-permalink" href="#Generic-optimization-interface" title="Permalink"></a></h2><p>The generic optimization interface is very general, handling gradients with respect to any parameter used in a function that sets up a complete simulation case from a <code>AbstractDict</code>. This makes use of <a href="https://github.com/JuliaDiff/AbstractDifferentiation.jl">AbstractDifferentiation.jl</a> and the default configuration assumes that your setup function can be differentiated with the <code>ForwardDiff</code> backend. In practice, this means that you must take care when initializing arrays and other types so that they can fit the AD type (e.g. avoid use of <code>zeros</code> without a type). In addition, there may be a large number of calls to the setup function, which can sometimes be slow. Alternatively, the numerical parameter optimization interface can be used, which only differentiates with respect to numerical parameters inside the model. A hybrid approach is also supported for the generic optimization interface by setting the <code>deps</code> and <code>deps_ad</code> arguments to <code>optimize</code>, which can be much faster, but assumes that the optimization variables only affect the numerical parameters/variables of the model (values stored in the Dicts from <code>setup_parameters</code> and <code>setup_state0</code>) and not any values that exist e.g. inside the model itself.</p><h3 id="Defining-the-parameter-object"><a class="docs-heading-anchor" href="#Defining-the-parameter-object">Defining the parameter object</a><a id="Defining-the-parameter-object-1"></a><a class="docs-heading-anchor-permalink" href="#Defining-the-parameter-object" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="Jutul.DictOptimization.DictParameters"><a class="docstring-binding" href="#Jutul.DictOptimization.DictParameters"><code>Jutul.DictOptimization.DictParameters</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">DictParameters(parameters)
DictParameters(parameters::AbstractDict, setup_function = missing;
        strict = true,
        verbose = true,
        active_type = Float64
    )</code></pre><p>Set up a <code>DictParameters</code> object for optimization. Optionally, the setup function that takes an instance with the same keys as <code>parameters</code> together with a <code>step_info</code> dictionary can be provided. The setup function should return a <code>JutulCase</code> set up from the parameters in the Dict.</p><p>Optional keyword arguments:</p><ul><li><code>strict</code>: If true, the optimization will throw an error if any of the parameters are not set with at least one of the upper or lower bounds.</li><li><code>verbose</code>: If true, the optimization will print information about the optimization process.</li><li><code>active_type</code>: The type of the parameters that are considered active in the optimization. Defaults to <code>Float64</code>. This is used to determine which parameters are active and should be optimized. This means that all entries (and entries in nested dictionaries) of the <code>parameters</code> dictionary must be of this type or an array with this type as element type.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sintefmath/Jutul.jl/blob/61db0547d056b6ca2f8db5edc1f198cb8015c64a/src/DictOptimization/types.jl#L66-L89">source</a></section></details></article><h3 id="Defining-constraints-and-free-parameters"><a class="docs-heading-anchor" href="#Defining-constraints-and-free-parameters">Defining constraints and free parameters</a><a id="Defining-constraints-and-free-parameters-1"></a><a class="docs-heading-anchor-permalink" href="#Defining-constraints-and-free-parameters" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="Jutul.DictOptimization.free_optimization_parameter!"><a class="docstring-binding" href="#Jutul.DictOptimization.free_optimization_parameter!"><code>Jutul.DictOptimization.free_optimization_parameter!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">free_optimization_parameter!(dopt, &quot;parameter_name&quot;, rel_min = 0.01, rel_max = 100.0)
free_optimization_parameter!(dopt, [&quot;dict_name&quot;, &quot;parameter_name&quot;], abs_min = -8.0, abs_max = 7.0)</code></pre><p>Free an existing parameter for optimization in the <code>DictParameters</code> object. This will allow the parameter to be optimized through a call to <a href="#Jutul.DictOptimization.optimize"><code>optimize</code></a>.</p><p><strong>Nesting structures</strong></p><p>If your <code>DictParameters</code> has a nesting structure, you can use a vector of strings or symbols to specify the parameter name, e.g. <code>[&quot;dict_name&quot;, &quot;parameter_name&quot;]</code> to access the parameter located at <code>[&quot;dict_name&quot;][&quot;parameter_name&quot;]</code>.</p><p><strong>Setting limits</strong></p><p>The limits can be set using the following keyword arguments:</p><ul><li><code>abs_min</code>: Absolute minimum value for the parameter.  If not set, no absolute minimum will be applied.</li><li><code>abs_max</code>: Absolute maximum value for the parameter. If not set, no absolute maximum will be applied.</li><li><code>rel_min</code>: Relative minimum value for the parameter. If not set, no relative minimum will be applied.</li><li><code>rel_max</code>: Relative maximum value for the parameter. If not set, no relative maximum will be applied.</li></ul><p>For either of these entries it is possible to pass either a scalar, or an array. If an array is passed, it must have the same size as the parameter being set.</p><p>Note that if <code>dopt.strict</code> is set to <code>true</code>, at least one of the upper or lower bounds must be set for free parameters. If <code>dopt.strict</code> is set to <code>false</code>, the bounds are optional and the <code>DictParameters</code> object can be used to compute sensitivities, but the built-in optimization routine assumes that finite limits are set for all parameters.</p><p><strong>Other keyword arguments</strong></p><ul><li><code>initial</code>: Initial value for the parameter. If not set, the current value in <code>dopt.parameters</code> will be used.</li><li><code>scaler=missing</code>: Optional scaler for the parameter. If not set, no scaling will be applied. Available scalers are:<ul><li><code>:log</code>: Logarithmic scaling. This value uses shifts to avoid issues with zero values.</li><li><code>:exp</code>: Exponential scaling</li><li><code>:linear</code>: Linear scaling (scaling to bounds of values, guaranteeing values between between 0 and 1 for initial values.)</li><li><code>linear_limits</code>: Linear scaling with limits (scaling to bounds of values, guaranteeing values between between 0 and 1 for all values within the limits.)</li><li><code>reciprocal</code>: Reciprocal scaling</li><li><code>log10</code>: Base-10 logarithmic scaling</li><li><code>log</code>: Base-e logarithmic scaling without shifts</li><li>A custom scaler object implementing the <code>DictOptimizationScaler</code> interface.</li></ul></li><li><code>lumping=missing</code>: Optional lumping array for the parameter. If not set, no lumping will be applied. The lumping array should have the same size as the parameter and contain positive integers. The lumping array defines groups of indices that should be lumped together, i.e. the same value will be used for all indices in the same group. The lumping array should contain all integers from 1 to the maximum value in the array, and all indices in the same group should have the same value in the initial parameter, otherwise an error will be thrown.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sintefmath/Jutul.jl/blob/61db0547d056b6ca2f8db5edc1f198cb8015c64a/src/DictOptimization/interface.jl#L387-L445">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jutul.DictOptimization.freeze_optimization_parameter!"><a class="docstring-binding" href="#Jutul.DictOptimization.freeze_optimization_parameter!"><code>Jutul.DictOptimization.freeze_optimization_parameter!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">freeze_optimization_parameter!(dopt, &quot;parameter_name&quot;)
freeze_optimization_parameter!(dopt, [&quot;dict_name&quot;, &quot;parameter_name&quot;])
freeze_optimization_parameter!(dopt::DictParameters, parameter_name, val = missing)</code></pre><p>Freeze an optimization parameter in the <code>DictParameters</code> object. This will remove the parameter from the optimization targets and set its value to <code>val</code> if provided. Any limits/lumping/scaling settings for this parameter will be removed.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sintefmath/Jutul.jl/blob/61db0547d056b6ca2f8db5edc1f198cb8015c64a/src/DictOptimization/interface.jl#L369-L378">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jutul.DictOptimization.set_optimization_parameter!"><a class="docstring-binding" href="#Jutul.DictOptimization.set_optimization_parameter!"><code>Jutul.DictOptimization.set_optimization_parameter!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">set_optimization_parameter!(dopt::DictParameters, parameter_name, value)</code></pre><p>Set a specific optimization parameter in the <code>DictParameters</code> object. This function will update the value of the parameter in the <code>dopt.parameters</code> dictionary.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sintefmath/Jutul.jl/blob/61db0547d056b6ca2f8db5edc1f198cb8015c64a/src/DictOptimization/interface.jl#L530-L535">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jutul.DictOptimization.add_optimization_multiplier!"><a class="docstring-binding" href="#Jutul.DictOptimization.add_optimization_multiplier!"><code>Jutul.DictOptimization.add_optimization_multiplier!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">add_optimization_multiplier!(dprm::DictParameters, name_of_target; abs_min = 0.2, abs_max = 5.0)
add_optimization_multiplier!(dprm, target1, target2, target3; abs_min = 0.2, abs_max = 5.0, initial = 2.0)</code></pre><p>Add an optimization multiplier that acts on one or more targets to the <code>DictParameters</code> object. The multiplier will be optimized during the optimization process. All parameters with the same multiplier must have the same dimensions.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sintefmath/Jutul.jl/blob/61db0547d056b6ca2f8db5edc1f198cb8015c64a/src/DictOptimization/interface.jl#L540-L548">source</a></section></details></article><h3 id="Optimizing-and-computing-gradients"><a class="docs-heading-anchor" href="#Optimizing-and-computing-gradients">Optimizing and computing gradients</a><a id="Optimizing-and-computing-gradients-1"></a><a class="docs-heading-anchor-permalink" href="#Optimizing-and-computing-gradients" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="Jutul.DictOptimization.optimize"><a class="docstring-binding" href="#Jutul.DictOptimization.optimize"><code>Jutul.DictOptimization.optimize</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">optimized_dict = optimize(dopt, objective)
optimize(dopt::DictParameters, objective, setup_fn = dopt.setup_function;
    grad_tol = 1e-6,
    obj_change_tol = 1e-6,
    max_it = 25,
    opt_fun = missing,
    maximize = false,
    simulator = missing,
    config = missing,
    solution_history = false,
    backend_arg = (
        use_sparsity = false,
        di_sparse = true,
        single_step_sparsity = false,
        do_prep = true,
    ),
    kwarg...
)</code></pre><p>Optimize parameters defined in a <a href="#Jutul.DictOptimization.DictParameters"><code>DictParameters</code></a> object using the provided objective function. At least one variable has to be declared to be free using <code>free_optimization_parameter!</code> prior to calling the optimizer.</p><p><strong>Arguments</strong></p><ul><li><code>dopt::DictParameters</code>: Container with parameters to optimize</li><li><code>objective</code>: The objective function to minimize (or maximize)</li><li><code>setup_fn</code>: Function to set up the optimization problem. Defaults to <code>dopt.setup_function</code></li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>grad_tol</code>: Gradient tolerance for stopping criterion</li><li><code>obj_change_tol</code>: Objective function change tolerance for stopping criterion</li><li><code>max_it</code>: Maximum number of iterations</li><li><code>optimizer</code>: Symbol defining the optimization algorithm to use. Available options are <code>:lbfgs</code> (default), <code>:lbfgsb_qp</code> and <code>:lbfgsb</code> (requires LBFGSB.jl to be imported)</li><li><code>opt_fun</code>: Optional custom optimization function. If missing, L-BFGS will be used. Takes in a NamedTuple containing fields <code>f</code>, <code>g</code>, <code>x0</code>, <code>min</code>, <code>max</code>. Here, <code>f(x)</code> returns the objective function value at <code>x</code>, <code>g(dFdx, x)</code> fills <code>dFdx</code> with the gradient at <code>x</code>, <code>x0</code> is the initial guess, and <code>min</code> and <code>max</code> are the lower and upper bounds, respectively. The functions <code>u = F.scale(x)</code> and <code>x = F.descale(u)</code> can be used to convert between scaled and unscaled variables. Nominally, the initial values are scaled to the unit cube and the solution must thus be unscaled before usage. Gradients and internal scaling/descaling is automatically handled.</li><li><code>maximize</code>: Set to <code>true</code> to maximize the objective instead of minimizing</li><li><code>gradient_scaling</code>: If <code>true</code>, internally scales the objective gradient according to the initial 2-norm of the gradient. If a <code>Float64</code> value is provided, that value is used as a global scaling factor for the gradient. The internal gradient and objective value is divided by the chosen scaling.</li><li><code>simulator</code>: Optional simulator object used in forward simulations</li><li><code>config</code>: Optional configuration for the setup</li><li><code>solution_history</code>: If <code>true</code>, stores all intermediate solutions</li><li><code>deps</code>: One of <code>:case</code>, <code>:parameters</code>, <code>:parameters_and_state0</code>. Defines the dependencies for the adjoint computation. See notes for more details.</li><li><code>backend_arg</code>: Options for the autodiff backend:<ul><li><code>use_sparsity</code>: Enable sparsity detection for the objective function</li><li><code>di_sparse</code>: Use sparse differentiation</li><li><code>single_step_sparsity</code>: Enable single step sparsity detection (if sparsity does not change during timesteps). This means that the solver will assume that the sparsity pattern will be determined entirely by the first and last steps of the simulation. Alternatively, this can be set to <code>:unique_forces</code> to use the sparsity pattern determined by all unique force terms in the solve, <code>:firstlast</code> to only use the first and last time steps or <code>:allsteps</code> to use all time steps (the latter is equivalent to setting <code>use_sparsity</code> to <code>true</code>).</li><li><code>do_prep</code>: Perform preparation step</li><li><code>output_path</code>: If provided, the optimization results will be stored in the given path as a JLD2 file named <code>final.jld2</code>, with intermediate steps being stored as <code>step_1.jld2</code>, <code>step_2.jld2</code>, etc if <code>solution_history</code> is enabled.</li></ul></li></ul><p><strong>Returns</strong></p><p>The optimized parameters as a dictionary.</p><p><strong>Notes</strong></p><ul><li>The function stores the optimization history and optimized parameters in the input <code>dopt</code> object.</li><li>If <code>solution_history</code> is <code>true</code> or :x, intermediate solutions are stored in <code>dopt.history.solutions</code>. If it is set to <code>:full</code>, the full states are also copied and stored for each iteration. This can use a lot of memory for large simulations.</li><li>The default optimization algorithm is L-BFGS with box constraints.</li></ul><p><strong>Type of dependencies in <code>deps</code></strong></p><p>The <code>deps</code> argument is used to set the type of dependency the <code>case</code> setup function has on the active optimization parameters. The default, <code>:case</code>, is fully general and allows dependencies on everything contained within the <code>case</code> instance. This can be slow, however, as the setup function must be called for every time-step. If you know that the model instance and forces are independent of the active parameters, you can use <code>deps = :parameters_and_state0</code>. If there is no dependence on <code>state0</code>, you can set <code>deps = :parameters</code>. This can substantially speed up the optimization process, but as there is no programmatic verification that this assumption is true, it should be used with care.</p><p>This interface is dependent on the model supporting use of <code>vectorize_variables!</code> and <code>devectorize_variables!</code> for <code>state0/parameters</code>, which should be the case for most Jutul models.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sintefmath/Jutul.jl/blob/61db0547d056b6ca2f8db5edc1f198cb8015c64a/src/DictOptimization/interface.jl#L1-L98">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jutul.DictOptimization.parameters_gradient"><a class="docstring-binding" href="#Jutul.DictOptimization.parameters_gradient"><code>Jutul.DictOptimization.parameters_gradient</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">parameters_gradient(dopt::DictParameters, objective, setup_fn = dopt.setup_function)</code></pre><p>Compute the gradient of the objective function with respect to the parameters defined in the <code>DictParameters</code> object. This function will return the gradient as a dictionary with the same structure as the input parameters, where each entry is a vector of gradients for each parameter. Only gradients with respect to free parameters will be computed.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sintefmath/Jutul.jl/blob/61db0547d056b6ca2f8db5edc1f198cb8015c64a/src/DictOptimization/interface.jl#L322-L330">source</a></section></details></article><h2 id="Numerical-parameter-optimization-interface"><a class="docs-heading-anchor" href="#Numerical-parameter-optimization-interface">Numerical parameter optimization interface</a><a id="Numerical-parameter-optimization-interface-1"></a><a class="docs-heading-anchor-permalink" href="#Numerical-parameter-optimization-interface" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="Jutul.solve_adjoint_sensitivities"><a class="docstring-binding" href="#Jutul.solve_adjoint_sensitivities"><code>Jutul.solve_adjoint_sensitivities</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">solve_adjoint_sensitivities(model, states, reports_or_timesteps, G; extra_timing = false, state0 = setup_state(model), forces = setup_forces(model), raw_output = false, kwarg...)</code></pre><p>Compute sensitivities of <code>model</code> parameter with name <code>target</code> for objective function <code>G</code>.</p><p>The objective function is at the moment assumed to be a sum over all states on the form: <code>obj = Σₙ G(model, state, dt_n, n, forces_for_step_n)</code></p><p>Solves the adjoint equations: For model equations F the gradient with respect to parameters p is     ∇ₚG = Σₙ (∂Fₙ / ∂p)ᵀ λₙ where n ∈ [1, N]. Given Lagrange multipliers λₙ from the adjoint equations     (∂Fₙ / ∂xₙ)ᵀ λₙ = - (∂J / ∂xₙ)ᵀ - (∂Fₙ₊₁ / ∂xₙ)ᵀ λₙ₊₁ where the last term is omitted for step n = N and G is the objective function.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sintefmath/Jutul.jl/blob/61db0547d056b6ca2f8db5edc1f198cb8015c64a/src/ad/gradients.jl#L3-L16">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jutul.solve_adjoint_sensitivities!"><a class="docstring-binding" href="#Jutul.solve_adjoint_sensitivities!"><code>Jutul.solve_adjoint_sensitivities!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">solve_adjoint_sensitivities!(∇G, storage, states, state0, timesteps, G; forces)</code></pre><p>Non-allocating version of <code>solve_adjoint_sensitivities</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sintefmath/Jutul.jl/blob/61db0547d056b6ca2f8db5edc1f198cb8015c64a/src/ad/gradients.jl#L225-L229">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jutul.solve_numerical_sensitivities"><a class="docstring-binding" href="#Jutul.solve_numerical_sensitivities"><code>Jutul.solve_numerical_sensitivities</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">solve_numerical_sensitivities(model, states, reports, G, target;
                                            forces = setup_forces(model),
                                            state0 = setup_state(model),
                                            parameters = setup_parameters(model),
                                            epsilon = 1e-8)</code></pre><p>Compute sensitivities of <code>model</code> parameter with name <code>target</code> for objective function <code>G</code>.</p><p>This method uses numerical perturbation and is primarily intended for testing of <code>solve_adjoint_sensitivities</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sintefmath/Jutul.jl/blob/61db0547d056b6ca2f8db5edc1f198cb8015c64a/src/ad/gradients.jl#L689-L699">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Jutul.setup_parameter_optimization"><a class="docstring-binding" href="#Jutul.setup_parameter_optimization"><code>Jutul.setup_parameter_optimization</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">setup_parameter_optimization(model, state0, param, dt, forces, G, opt_cfg = optimization_config(model, param);
    grad_type = :adjoint,
    config = nothing,
    print = 1,
    copy_case = true,
    param_obj = false,
    kwarg...
)</code></pre><p>Set up function handles for optimizing the case defined by the inputs to <code>simulate</code> together with a per-timestep objective function <code>G</code>. The objective function should fit the format described in <a href="#Jutul.AbstractGlobalObjective">AbstractGlobalObjective</a> or <a href="#Jutul.AbstractSumObjective">AbstractSumObjective</a>.</p><p>Generally calling either of the functions will mutate the data Dict. The options are:</p><ul><li><code>F_o(x)</code>: evaluate objective</li><li><code>dF_o(dFdx, x)</code>: evaluate gradient of objective, mutating <code>dFdx</code> (may trigger evaluation of <code>F_o</code>)</li><li><code>F_and_dF(F, dFdx, x)</code>: evaluate <code>F</code> and/or <code>dF</code>. If <code>nothing</code> is passed for an entry, the corresponding entry is skipped.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sintefmath/Jutul.jl/blob/61db0547d056b6ca2f8db5edc1f198cb8015c64a/src/simulator/optimization.jl#L17-L39">source</a></section></details></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../usage/">« Usage</a><a class="docs-footer-nextpage" href="../mesh/">Meshes and mesh utilities »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Wednesday 4 February 2026 06:59">Wednesday 4 February 2026</span>. Using Julia version 1.12.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
